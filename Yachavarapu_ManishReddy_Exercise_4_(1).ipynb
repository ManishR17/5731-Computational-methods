{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "\n",
        "**Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d500f51e-efa2-4c8c-828b-5d2e0ccb8a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of topics: 5\n",
            "\n",
            "Topics and Top Words:\n",
            "Topic 1: film | movie | harry | potter | book\n",
            "Topic 2: harry | movie | potter | film | book\n",
            "Topic 3: film | movie | harry | potter | book\n",
            "Topic 4: film | movie | book | potter | harry\n",
            "Topic 5: movie | harry | film | potter | book\n",
            "\n",
            "Topics Summary:\n",
            "Topic 1: film | movie | harry | potter | book\n",
            "Topic 2: harry | movie | potter | film | book\n",
            "Topic 3: film | movie | harry | potter | book\n",
            "Topic 4: film | movie | book | potter | harry\n",
            "Topic 5: movie | harry | film | potter | book\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "from collections import Counter\n",
        "\n",
        "# Function to preprocess text data for topic modeling\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]  # Remove stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize tokens\n",
        "    return tokens\n",
        "\n",
        "# Function to scrape IMDb reviews\n",
        "def scrape_imdb_reviews(url, max_reviews=50):\n",
        "    reviews = []\n",
        "    page_num = 1\n",
        "\n",
        "    while len(reviews) < max_reviews:\n",
        "        page_url = f\"{url}&start={page_num * 10 - 10}\"\n",
        "        response = requests.get(page_url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            review_containers = soup.find_all('div', class_='lister-item-content')\n",
        "\n",
        "            if not review_containers:\n",
        "                print(\"No more reviews available.\")\n",
        "                break\n",
        "\n",
        "            for container in review_containers:\n",
        "                review_text = container.find('div', class_='text show-more__control').get_text().strip()\n",
        "                reviews.append(review_text)\n",
        "\n",
        "                if len(reviews) >= max_reviews:\n",
        "                    break\n",
        "\n",
        "            page_num += 1\n",
        "        else:\n",
        "            print(f\"Failed to fetch IMDb reviews page {page_num}. Status code:\", response.status_code)\n",
        "            break\n",
        "\n",
        "    return reviews[:max_reviews]\n",
        "\n",
        "# URL of the IMDb page to scrape reviews from\n",
        "url = 'https://www.imdb.com/title/tt0241527/reviews?ref_=tt_urv'\n",
        "reviews = scrape_imdb_reviews(url, max_reviews=50)\n",
        "\n",
        "# Preprocess reviews for LDA topic modeling\n",
        "preprocessed_reviews = [preprocess_text(review) for review in reviews]\n",
        "\n",
        "# Create a dictionary from the preprocessed reviews\n",
        "dictionary = Dictionary(preprocessed_reviews)\n",
        "\n",
        "# Convert the reviews to bag-of-words format\n",
        "corpus = [dictionary.doc2bow(review) for review in preprocessed_reviews]\n",
        "\n",
        "# Determine the optimal number of topics (K) based on coherence score\n",
        "coherence_scores = {}\n",
        "for k in range(2, 11):\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, random_state=42)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_reviews, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model_lda.get_coherence()\n",
        "    coherence_scores[k] = coherence_score\n",
        "\n",
        "# Choose the number of topics with the highest coherence score\n",
        "optimal_k = max(coherence_scores, key=coherence_scores.get)\n",
        "print(\"Optimal number of topics:\", optimal_k)\n",
        "\n",
        "# Train the LDA model with the optimal number of topics\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=optimal_k, random_state=42)\n",
        "\n",
        "# Get the top words for each topic\n",
        "topic_top_words = {}\n",
        "for topic_id in range(optimal_k):\n",
        "    topic_words = lda_model.get_topic_terms(topic_id, topn=5)\n",
        "    top_words = [dictionary[word_id] for word_id, _ in topic_words]\n",
        "    topic_top_words[topic_id] = top_words\n",
        "\n",
        "# Print the topics and their top words\n",
        "print(\"\\nTopics and Top Words:\")\n",
        "for topic, top_words in topic_top_words.items():\n",
        "    print(f\"Topic {topic + 1}: {' | '.join(top_words)}\")\n",
        "\n",
        "# Function to summarize topics based on top words\n",
        "def summarize_topics(topic_top_words):\n",
        "    topics_summary = []\n",
        "    for topic, top_words in topic_top_words.items():\n",
        "        topics_summary.append(f\"Topic {topic + 1}: {' | '.join(top_words)}\")\n",
        "    return '\\n'.join(topics_summary)\n",
        "\n",
        "# Summarize the topics\n",
        "topics_summary = summarize_topics(topic_top_words)\n",
        "print(\"\\nTopics Summary:\")\n",
        "print(topics_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "\n",
        "**Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "344670bd-3b0a-45f8-eaf8-bbcd1e62f3a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of topics: 8\n",
            "\n",
            "Topics and Top Words:\n",
            "Topic 1: movie | harry | film | potter | book | like | child | character | first | time\n",
            "Topic 2: movie | film | harry | also | wizard | magic | rowling | though | boy | villain\n",
            "Topic 3: book | harry | potter | story | film | also | stone | way | much | even\n",
            "Topic 4: philosopher | like | special | would | potter | good | thing | really | kid | stone\n",
            "Topic 5: first | kid | world | see | harry | also | special | movie | magic | richard\n",
            "Topic 6: film | like | would | also | without | special | feel | scene | new | want\n",
            "Topic 7: nothing | actor | young | harry | read | character | role | novel | want | fine\n",
            "Topic 8: feel | book | film | harry | never | potter | scene | wizard | also | like\n",
            "\n",
            "Topics Summary:\n",
            "Topic 1: movie | harry | film | potter | book | like | child | character | first | time\n",
            "Topic 2: movie | film | harry | also | wizard | magic | rowling | though | boy | villain\n",
            "Topic 3: book | harry | potter | story | film | also | stone | way | much | even\n",
            "Topic 4: philosopher | like | special | would | potter | good | thing | really | kid | stone\n",
            "Topic 5: first | kid | world | see | harry | also | special | movie | magic | richard\n",
            "Topic 6: film | like | would | also | without | special | feel | scene | new | want\n",
            "Topic 7: nothing | actor | young | harry | read | character | role | novel | want | fine\n",
            "Topic 8: feel | book | film | harry | never | potter | scene | wizard | also | like\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LsiModel, CoherenceModel\n",
        "from collections import Counter\n",
        "\n",
        "# Function to preprocess text data for topic modeling\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]  # Remove stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize tokens\n",
        "    return tokens\n",
        "\n",
        "# Function to scrape IMDb reviews\n",
        "def scrape_imdb_reviews(url, max_reviews=50):\n",
        "    reviews = []\n",
        "    page_num = 1\n",
        "\n",
        "    while len(reviews) < max_reviews:\n",
        "        page_url = f\"{url}&start={page_num * 10 - 10}\"\n",
        "        response = requests.get(page_url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            review_containers = soup.find_all('div', class_='lister-item-content')\n",
        "\n",
        "            if not review_containers:\n",
        "                print(\"No more reviews available.\")\n",
        "                break\n",
        "\n",
        "            for container in review_containers:\n",
        "                review_text = container.find('div', class_='text show-more__control').get_text().strip()\n",
        "                reviews.append(review_text)\n",
        "\n",
        "                if len(reviews) >= max_reviews:\n",
        "                    break\n",
        "\n",
        "            page_num += 1\n",
        "        else:\n",
        "            print(f\"Failed to fetch IMDb reviews page {page_num}. Status code:\", response.status_code)\n",
        "            break\n",
        "\n",
        "    return reviews[:max_reviews]\n",
        "\n",
        "# URL of the IMDb page to scrape reviews from\n",
        "url = 'https://www.imdb.com/title/tt0241527/reviews?ref_=tt_urv'\n",
        "reviews = scrape_imdb_reviews(url, max_reviews=50)\n",
        "\n",
        "# Preprocess reviews for LSA topic modeling\n",
        "preprocessed_reviews = [preprocess_text(review) for review in reviews]\n",
        "\n",
        "# Create a dictionary from the preprocessed reviews\n",
        "dictionary = Dictionary(preprocessed_reviews)\n",
        "\n",
        "# Convert the reviews to bag-of-words format\n",
        "corpus = [dictionary.doc2bow(review) for review in preprocessed_reviews]\n",
        "\n",
        "# Determine the optimal number of topics (K) based on coherence score\n",
        "coherence_scores = {}\n",
        "for k in range(2, 11):\n",
        "    lsa_model = LsiModel(corpus=corpus, id2word=dictionary, num_topics=k)\n",
        "    coherence_model_lsa = CoherenceModel(model=lsa_model, texts=preprocessed_reviews, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model_lsa.get_coherence()\n",
        "    coherence_scores[k] = coherence_score\n",
        "\n",
        "# Choose the number of topics with the highest coherence score\n",
        "optimal_k = max(coherence_scores, key=coherence_scores.get)\n",
        "print(\"Optimal number of topics:\", optimal_k)\n",
        "\n",
        "# Train the LSA model with the optimal number of topics\n",
        "lsa_model = LsiModel(corpus=corpus, id2word=dictionary, num_topics=optimal_k)\n",
        "\n",
        "# Get the top words for each topic\n",
        "topic_top_words = {}\n",
        "for topic_id, topic in lsa_model.show_topics(formatted=False):\n",
        "    top_words = [word for word, _ in topic]\n",
        "    topic_top_words[topic_id] = top_words\n",
        "\n",
        "# Print the topics and their top words\n",
        "print(\"\\nTopics and Top Words:\")\n",
        "for topic, top_words in topic_top_words.items():\n",
        "    print(f\"Topic {topic + 1}: {' | '.join(top_words)}\")\n",
        "\n",
        "# Function to summarize topics based on top words\n",
        "def summarize_topics(topic_top_words):\n",
        "    topics_summary = []\n",
        "    for topic, top_words in topic_top_words.items():\n",
        "        topics_summary.append(f\"Topic {topic + 1}: {' | '.join(top_words)}\")\n",
        "    return '\\n'.join(topics_summary)\n",
        "\n",
        "# Summarize the topics\n",
        "topics_summary = summarize_topics(topic_top_words)\n",
        "print(\"\\nTopics Summary:\")\n",
        "print(topics_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RX33ndhiSXO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "**Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e31799b-5981-422d-fdf6-0d75afe15970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of topics: 2\n",
            "\n",
            "Topics and Top Words:\n",
            "Topic 1: movie | harry | potter | film | book | like | great | character | first | good\n",
            "Topic 2: movie | film | harry | potter | book | character | like | child | wizard | one\n",
            "\n",
            "Topics Summary:\n",
            "Topic 1: movie | harry | potter | film | book | like | great | character | first | good\n",
            "Topic 2: movie | film | harry | potter | book | character | like | child | wizard | one\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "from collections import Counter\n",
        "\n",
        "# Function to preprocess text data for topic modeling\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]  # Remove stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize tokens\n",
        "    return tokens\n",
        "\n",
        "# Function to scrape IMDb reviews\n",
        "def scrape_imdb_reviews(url, max_reviews=50):\n",
        "    reviews = []\n",
        "    page_num = 1\n",
        "\n",
        "    while len(reviews) < max_reviews:\n",
        "        page_url = f\"{url}&start={page_num * 10 - 10}\"\n",
        "        response = requests.get(page_url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            review_containers = soup.find_all('div', class_='lister-item-content')\n",
        "\n",
        "            if not review_containers:\n",
        "                print(\"No more reviews available.\")\n",
        "                break\n",
        "\n",
        "            for container in review_containers:\n",
        "                review_text = container.find('div', class_='text show-more__control').get_text().strip()\n",
        "                reviews.append(review_text)\n",
        "\n",
        "                if len(reviews) >= max_reviews:\n",
        "                    break\n",
        "\n",
        "            page_num += 1\n",
        "        else:\n",
        "            print(f\"Failed to fetch IMDb reviews page {page_num}. Status code:\", response.status_code)\n",
        "            break\n",
        "\n",
        "    return reviews[:max_reviews]\n",
        "\n",
        "# URL of the IMDb page to scrape reviews from\n",
        "url = 'https://www.imdb.com/title/tt0241527/reviews?ref_=tt_urv'\n",
        "reviews = scrape_imdb_reviews(url, max_reviews=50)\n",
        "\n",
        "# Preprocess reviews for LDA topic modeling\n",
        "preprocessed_reviews = [preprocess_text(review) for review in reviews]\n",
        "\n",
        "# Create a dictionary from the preprocessed reviews\n",
        "dictionary = Dictionary(preprocessed_reviews)\n",
        "\n",
        "# Convert the reviews to bag-of-words format\n",
        "corpus = [dictionary.doc2bow(review) for review in preprocessed_reviews]\n",
        "\n",
        "# Determine the optimal number of topics (K) based on coherence score\n",
        "coherence_scores = {}\n",
        "for k in range(2, 11):\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=k)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_reviews, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model_lda.get_coherence()\n",
        "    coherence_scores[k] = coherence_score\n",
        "\n",
        "# Choose the number of topics with the highest coherence score\n",
        "optimal_k = max(coherence_scores, key=coherence_scores.get)\n",
        "print(\"Optimal number of topics:\", optimal_k)\n",
        "\n",
        "# Train the LDA model with the optimal number of topics\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=optimal_k)\n",
        "\n",
        "# Get the top words for each topic\n",
        "topic_top_words = {}\n",
        "for topic_id, topic in lda_model.show_topics(formatted=False):\n",
        "    top_words = [word for word, _ in topic]\n",
        "    topic_top_words[topic_id] = top_words\n",
        "\n",
        "# Print the topics and their top words\n",
        "print(\"\\nTopics and Top Words:\")\n",
        "for topic, top_words in topic_top_words.items():\n",
        "    print(f\"Topic {topic + 1}: {' | '.join(top_words)}\")\n",
        "\n",
        "# Function to summarize topics based on top words\n",
        "def summarize_topics(topic_top_words):\n",
        "    topics_summary = []\n",
        "    for topic, top_words in topic_top_words.items():\n",
        "        topics_summary.append(f\"Topic {topic + 1}: {' | '.join(top_words)}\")\n",
        "    return '\\n'.join(topics_summary)\n",
        "\n",
        "# Summarize the topics\n",
        "topics_summary = summarize_topics(topic_top_words)\n",
        "print(\"\\nTopics Summary:\")\n",
        "print(topics_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "**Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63206536-ca3e-42bd-fadd-2d089198a62d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Topics and Top Words:\n",
            "Topic 0: ('harry', 0.08904450600962192) | ('film', 0.06861888795651858) | ('magic', 0.04653463597890399) | ('potter', 0.043116946571880574) | ('wizard', 0.04272198355777768)\n",
            "Topic 1: ('movie', 0.08578276013531544) | ('film', 0.0565590582428615) | ('harry', 0.051756965131882306) | ('potter', 0.04975474757827734) | ('book', 0.04110724687113712)\n",
            "Topic 2: ('great', 0.1145428279901484) | ('film', 0.0631336438447923) | ('good', 0.06248197933300945) | ('harry', 0.05391742870571163) | ('harris', 0.053792902033650954)\n",
            "\n",
            "Topics Summary:\n",
            "Topic 0: ('harry', 0.08904450600962192) | ('film', 0.06861888795651858) | ('magic', 0.04653463597890399) | ('potter', 0.043116946571880574) | ('wizard', 0.04272198355777768)\n",
            "Topic 1: ('movie', 0.08578276013531544) | ('film', 0.0565590582428615) | ('harry', 0.051756965131882306) | ('potter', 0.04975474757827734) | ('book', 0.04110724687113712)\n",
            "Topic 2: ('great', 0.1145428279901484) | ('film', 0.0631336438447923) | ('good', 0.06248197933300945) | ('harry', 0.05391742870571163) | ('harris', 0.053792902033650954)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.corpora import Dictionary\n",
        "from bertopic import BERTopic\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Install NLTK data if needed\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Function to preprocess text data for topic modeling\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]  # Remove stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize tokens\n",
        "    return tokens\n",
        "\n",
        "# Function to scrape IMDb reviews\n",
        "def scrape_imdb_reviews(url, max_reviews=50):\n",
        "    reviews = []\n",
        "    page_num = 1\n",
        "\n",
        "    while len(reviews) < max_reviews:\n",
        "        page_url = f\"{url}&start={page_num * 10 - 10}\"\n",
        "        response = requests.get(page_url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            review_containers = soup.find_all('div', class_='lister-item-content')\n",
        "\n",
        "            if not review_containers:\n",
        "                print(\"No more reviews available.\")\n",
        "                break\n",
        "\n",
        "            for container in review_containers:\n",
        "                review_text = container.find('div', class_='text show-more__control').get_text().strip()\n",
        "                reviews.append(review_text)\n",
        "\n",
        "                if len(reviews) >= max_reviews:\n",
        "                    break\n",
        "\n",
        "            page_num += 1\n",
        "        else:\n",
        "            print(f\"Failed to fetch IMDb reviews page {page_num}. Status code:\", response.status_code)\n",
        "            break\n",
        "\n",
        "    return reviews[:max_reviews]\n",
        "\n",
        "# URL of the IMDb page to scrape reviews from\n",
        "url = 'https://www.imdb.com/title/tt0241527/reviews?ref_=tt_urv'\n",
        "reviews = scrape_imdb_reviews(url, max_reviews=50)\n",
        "\n",
        "# Preprocess reviews for BERTopic\n",
        "preprocessed_reviews = [' '.join(preprocess_text(review)) for review in reviews]\n",
        "\n",
        "# Initialize BERTopic and determine the optimal number of topics (K) based on coherence score\n",
        "topic_model = BERTopic()\n",
        "topics, _ = topic_model.fit_transform(preprocessed_reviews)\n",
        "\n",
        "# Print the number of topics and their top words\n",
        "topic_top_words = topic_model.get_topics()\n",
        "print(\"\\nTopics and Top Words:\")\n",
        "for topic_id, top_words in topic_top_words.items():\n",
        "    print(f\"Topic {topic_id + 1}: {' | '.join(map(str, top_words[:5]))}\")  # Convert top_words to strings\n",
        "\n",
        "# Function to summarize topics based on top words\n",
        "def summarize_topics(topic_top_words):\n",
        "    topics_summary = []\n",
        "    for topic_id, top_words in topic_top_words.items():\n",
        "        topics_summary.append(f\"Topic {topic_id + 1}: {' | '.join(map(str, top_words[:5]))}\")\n",
        "    return '\\n'.join(topics_summary)\n",
        "\n",
        "# Summarize the topics\n",
        "topics_summary = summarize_topics(topic_top_words)\n",
        "print(\"\\nTopics Summary:\")\n",
        "print(topics_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 40 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "En resume, BERTopic and LDA seem to have the best performance regarding coherence and interpretability which is why they are becoming increasingly popular for different text analysis purposes.\n",
        " Especially LSA, is under the influence of singular value decomposition and so, it usually does not obtain coherence scores as high as those of humans and is rather hard to interpret.\n",
        "  LdA2Vec provides more accurate topic but needs additional adjustments and may be not successful to surpass the LDA because of their simplicity. There comes a question of the best algorithm and there are variants like coherence,\n",
        "  interpretability, and advanced context - aware modeling that will have to be done to know the best to use. In all, BERTopic and LDA are mostly chosen for the reason that they have got these two factors much into the balance.\n"
      ],
      "metadata": {
        "id": "OK34nZtojhmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "This assignment is challenging for me, i have learned bert and lda model and i learned how to generate k values using lda.\n",
        "i got clear understading about these algorithms and implementation form corpus.\n",
        "i faced issues with lda2ves implementation, my colab is not supporting when in install thje lda2vec, all files are\n",
        "not include in this version, i have to include missing files manually in conda implementation. so i used lda instead of lad2vec\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}