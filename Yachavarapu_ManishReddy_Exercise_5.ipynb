{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU-pLW33lpcS"
      },
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loi8Sh7UE6ha"
      },
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAZj4PHB70nf",
        "outputId": "1a7b5864-48ae-4723-ae9b-918d2d6edb88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating MultinomialNB...\n",
            "MultinomialNB - Evaluation Metrics:\n",
            "Accuracy: 0.7796218199385041\n",
            "Precision: 0.7596894214796257\n",
            "Recall: 0.847496892610612\n",
            "F1-score: 0.800788608822262\n",
            "\n",
            "Evaluating SVM...\n",
            "SVM - Evaluation Metrics:\n",
            "Accuracy: 0.775101350689707\n",
            "Precision: 0.770625897102649\n",
            "Recall: 0.8118251646496912\n",
            "F1-score: 0.7903445472649195\n",
            "\n",
            "Evaluating KNN...\n",
            "KNN - Evaluation Metrics:\n",
            "Accuracy: 0.7156788374537312\n",
            "Precision: 0.7228559332510321\n",
            "Recall: 0.7402719138687788\n",
            "F1-score: 0.7310618689469263\n",
            "\n",
            "Evaluating DecisionTree...\n",
            "DecisionTree - Evaluation Metrics:\n",
            "Accuracy: 0.6065749015870114\n",
            "Precision: 0.6172270540339757\n",
            "Recall: 0.6313611807277828\n",
            "F1-score: 0.6247996176688593\n",
            "\n",
            "Evaluating RandomForest...\n",
            "RandomForest - Evaluation Metrics:\n",
            "Accuracy: 0.7032079696568112\n",
            "Precision: 0.6998856525045852\n",
            "Recall: 0.7629514519541646\n",
            "F1-score: 0.7260360113561232\n",
            "\n",
            "Evaluating XGBoost...\n",
            "XGBoost - Evaluation Metrics:\n",
            "Accuracy: 0.6954472160385425\n",
            "Precision: 0.6897249222820749\n",
            "Recall: 0.7610451773100005\n",
            "F1-score: 0.7231364545985124\n",
            "\n",
            "Best model: MultinomialNB\n",
            "Final evaluation on validation data:\n",
            "Accuracy: 0.7933526011560693\n",
            "Precision: 0.7532621589561092\n",
            "Recall: 0.8906030855539971\n",
            "F1-score: 0.8161953727506427\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load data\n",
        "def load_data(file_path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line_num, line in enumerate(file, start=1):\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split(' ')\n",
        "                label = parts[0]\n",
        "                text = ' '.join(parts[1:])\n",
        "                try:\n",
        "                    labels.append(int(label))\n",
        "                    texts.append(text)\n",
        "                except ValueError:\n",
        "                    print(f\"Error parsing line {line_num}: '{line}'\")\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = load_data('stsa-train.txt')\n",
        "test_texts, test_labels = load_data('stsa-test.txt')\n",
        "\n",
        "# Split training data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'MultinomialNB': MultinomialNB(),\n",
        "    'SVM': SVC(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'DecisionTree': DecisionTreeClassifier(),\n",
        "    'RandomForest': RandomForestClassifier(),\n",
        "    'XGBoost': XGBClassifier()\n",
        "}\n",
        "\n",
        "# Initialize evaluation metrics\n",
        "evaluation_metrics = {\n",
        "    'Accuracy': accuracy_score,\n",
        "    'Precision': precision_score,\n",
        "    'Recall': recall_score,\n",
        "    'F1-score': f1_score\n",
        "}\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Transform the text data into TF-IDF features\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Perform 10-fold cross-validation and evaluate each model\n",
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    print(f'Evaluating {model_name}...')\n",
        "    # Perform cross-validation\n",
        "    kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    accuracy_scores = cross_val_score(model, X_train_tfidf, y_train, cv=kfold, scoring='accuracy')\n",
        "    precision_scores = cross_val_score(model, X_train_tfidf, y_train, cv=kfold, scoring='precision')\n",
        "    recall_scores = cross_val_score(model, X_train_tfidf, y_train, cv=kfold, scoring='recall')\n",
        "    f1_scores = cross_val_score(model, X_train_tfidf, y_train, cv=kfold, scoring='f1')\n",
        "\n",
        "    results[model_name] = {\n",
        "        'Accuracy': accuracy_scores.mean(),\n",
        "        'Precision': precision_scores.mean(),\n",
        "        'Recall': recall_scores.mean(),\n",
        "        'F1-score': f1_scores.mean()\n",
        "    }\n",
        "\n",
        "    print(f'{model_name} - Evaluation Metrics:')\n",
        "    print(f'Accuracy: {results[model_name][\"Accuracy\"]}')\n",
        "    print(f'Precision: {results[model_name][\"Precision\"]}')\n",
        "    print(f'Recall: {results[model_name][\"Recall\"]}')\n",
        "    print(f'F1-score: {results[model_name][\"F1-score\"]}')\n",
        "    print('')\n",
        "\n",
        "# Select the best-performing model based on accuracy\n",
        "best_model = max(results, key=lambda x: results[x]['Accuracy'])\n",
        "print(f'Best model: {best_model}')\n",
        "\n",
        "# Train the final model using the selected algorithm on the entire training data\n",
        "final_model = models[best_model]\n",
        "final_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "\n",
        "# Evaluate the final trained model on the validation data\n",
        "y_pred = final_model.predict(X_val_tfidf)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "precision = precision_score(y_val, y_pred)\n",
        "recall = recall_score(y_val, y_pred)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "print(f'Final evaluation on validation data:')\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1-score: {f1}')\n",
        "\n",
        "# Now you can proceed to evaluate on test data as well if required.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "EoQX5s4O70nf",
        "outputId": "5863cd9c-5eba-484d-9eb4-ce1b9db0b22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-means Runtime: 2.843510627746582\n",
            "K-means Silhouette Score: 0.03825677867008131\n",
            "DBSCAN Runtime: 3.512040853500366\n",
            "DBSCAN Silhouette Score: -0.1589411172861913\n",
            "Hierarchical Runtime: 15.18751311302185\n",
            "Hierarchical Silhouette Score: 0.017666229535585123\n",
            "Word2Vec Runtime: 2.346010208129883\n",
            "Word2Vec Silhouette Score: 0.5161927\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(\"BERT Runtime:\", bert_time)\\nprint(\"BERT Silhouette Score:\", silhouette_bert)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import time\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
        "\n",
        "# For demonstration purposes, let's use a sample of the dataset\n",
        "data = data.sample(frac=0.05, random_state=42)\n",
        "\n",
        "# Preprocess text data\n",
        "data.dropna(subset=['Reviews'], inplace=True)\n",
        "text_data = data['Reviews'].values\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X = vectorizer.fit_transform(text_data)\n",
        "\n",
        "# K-means clustering\n",
        "start_time = time.time()\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X)\n",
        "kmeans_time = time.time() - start_time\n",
        "silhouette_kmeans = silhouette_score(X, kmeans_labels)\n",
        "\n",
        "# DBSCAN clustering\n",
        "start_time = time.time()\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(X)\n",
        "dbscan_time = time.time() - start_time\n",
        "silhouette_dbscan = silhouette_score(X, dbscan_labels)\n",
        "\n",
        "# Hierarchical clustering\n",
        "start_time = time.time()\n",
        "hierarchical = AgglomerativeClustering(n_clusters=5)\n",
        "hierarchical_labels = hierarchical.fit_predict(X.toarray())\n",
        "hierarchical_time = time.time() - start_time\n",
        "silhouette_hierarchical = silhouette_score(X, hierarchical_labels)\n",
        "\n",
        "# Word2Vec clustering\n",
        "start_time = time.time()\n",
        "word2vec_model = Word2Vec(sentences=[review.split() for review in text_data], vector_size=100, window=5, min_count=1, workers=4)\n",
        "word2vec_features = np.array([word2vec_model.wv[review.split()].mean(axis=0) for review in text_data])\n",
        "kmeans_word2vec = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans_labels_word2vec = kmeans_word2vec.fit_predict(word2vec_features)\n",
        "word2vec_time = time.time() - start_time\n",
        "silhouette_word2vec = silhouette_score(word2vec_features, kmeans_labels_word2vec)\n",
        "\n",
        "# BERT clustering\n",
        "'''\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "embeddings = []\n",
        "start_time = time.time()\n",
        "for review in text_data:\n",
        "    inputs = tokenizer(review, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    outputs = model(**inputs)\n",
        "    embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy())\n",
        "embeddings = np.array(embeddings)\n",
        "pca = TruncatedSVD(n_components=100)\n",
        "embeddings_pca = pca.fit_transform(embeddings)\n",
        "kmeans_bert = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans_labels_bert = kmeans_bert.fit_predict(embeddings_pca)\n",
        "bert_time = time.time() - start_time\n",
        "silhouette_bert = silhouette_score(embeddings_pca, kmeans_labels_bert)\n",
        "'''\n",
        "# Print runtime and silhouette score for each clustering algorithm\n",
        "print(\"K-means Runtime:\", kmeans_time)\n",
        "print(\"K-means Silhouette Score:\", silhouette_kmeans)\n",
        "print(\"DBSCAN Runtime:\", dbscan_time)\n",
        "print(\"DBSCAN Silhouette Score:\", silhouette_dbscan)\n",
        "print(\"Hierarchical Runtime:\", hierarchical_time)\n",
        "print(\"Hierarchical Silhouette Score:\", silhouette_hierarchical)\n",
        "print(\"Word2Vec Runtime:\", word2vec_time)\n",
        "print(\"Word2Vec Silhouette Score:\", silhouette_word2vec)\n",
        "'''\n",
        "print(\"BERT Runtime:\", bert_time)\n",
        "print(\"BERT Silhouette Score:\", silhouette_bert)'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDrsWudtlSu5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRijW2aLGONl"
      },
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The effectiveness of K-means and Hierarchical clustering in clustering was found to be restricted, since low silhouette scores indicated ineffective cluster separation. DBSCAN received a negative silhouette score because it was unable to detect dense regions in an efficient manner. Word2Vec demonstrated superior semantic embedding as seen by its high silhouette score, outperforming other clustering techniques. Though not directly comparable for clustering, BERT, which is intended for contextualized embeddings, outperforms Word2Vec in capturing complicated contextual links. Overall, embedding approaches like Word2Vec and BERT demonstrated greater skills in capturing semantic subtleties, whereas standard clustering methods faltered.\n"
      ],
      "metadata": {
        "id": "_Mv486Pwaryc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIYCj5qyGfSL"
      },
      "source": [
        "**Write your response here:**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEs-OoDEhTW4"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUKC7suYhVl0"
      },
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "this exercise was most challenging compared to all exercises, the dificult task is runtime, to rectify the error and\n",
        "to rerun the code to implement all models it taking too long.\n",
        "And in first question i can’t implement the Word2Vec and Bert becuase they are not classifiers\n",
        "in second question Bert model is taking too long to run, i have waited for 50 minutes but still its not executed\n",
        "so i put the Bert model in comments and executed other models\n",
        "And Bert and Word2Vec are not clustering model, so i faced main problem their.\n",
        "\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}