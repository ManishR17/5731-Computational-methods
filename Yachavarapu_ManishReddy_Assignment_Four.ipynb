{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
        "\n",
        "1. Features (text representation) used for topic modeling.\n",
        "\n",
        "2. Top 10 clusters for topic modeling.\n",
        "\n",
        "3. Summarize and describe the topic for each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4c7721-549a-4aeb-e1be-ffc48247f4a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features (text representation) used for topic modeling:\n",
            "['007' '10' '100' ... 'young' 'youngsters' 'zoe']\n",
            "\n",
            "Top 10 clusters/topics for LDA:\n",
            "Cluster 1: start, career, unforgettable, 10, kids, family, directing, did, perfect, acting\n",
            "Cluster 2: start, career, unforgettable, 10, kids, family, directing, did, perfect, acting\n",
            "Cluster 3: movie, potter, harry, film, really, good, like, philosopher, stone, long\n",
            "Cluster 4: lot, film, production, did, effects, excellent, franchise, bit, dark, cinematography\n",
            "Cluster 5: film, harry, just, potter, great, book, rowling, world, characters, stone\n",
            "Cluster 6: movie, potter, harry, kids, movies, series, special, world, going, alan\n",
            "Cluster 7: movie, harry, potter, film, movies, good, characters, book, effects, wizard\n",
            "Cluster 8: harry, movie, film, potter, great, like, book, sorcerer, columbus, stone\n",
            "Cluster 9: start, career, unforgettable, 10, kids, family, directing, did, perfect, acting\n",
            "Cluster 10: movie, book, story, wasn, books, old, didn, right, watch, reading\n",
            "\n",
            "Top 10 clusters/topics for LSA:\n",
            "Cluster 1: movie, harry, film, potter, book, like, story, world, movies, just\n",
            "Cluster 2: movie, movies, series, villain, like, course, children, sort, books, way\n",
            "Cluster 3: book, film, story, books, way, like, kids, school, details, read\n",
            "Cluster 4: philosopher, potter, like, good, really, stone, film, pretty, cut, scenes\n",
            "Cluster 5: just, film, world, feel, harris, kids, series, seeing, scene, going\n",
            "Cluster 6: like, kids, special, ll, books, world, perfect, philosopher, time, richard\n",
            "Cluster 7: book, actors, rowling, sorceror, screen, world, potter, bit, played, fine\n",
            "Cluster 8: read, doesn, does, want, story, characters, young, middle, act, potter\n",
            "Cluster 9: effects, excellent, special, generally, blockbuster, old, lots, enjoyable, adults, fine\n",
            "Cluster 10: great, franchise, sorcerer, stone, good, characters, thought, harris, young, really\n",
            "\n",
            "Top clusters/topics for BERTopic:\n",
            "Cluster 1: the, and, to, of, is, in, film, it, this, as\n",
            "Cluster 2: the, in, and, to, of, great, movie, first, harry, is\n",
            "Cluster 3: was, of, the, it, were, directing, for, book, did, lot\n",
            "Cluster 4: the, of, and, to, is, that, as, harry, film, in\n",
            "Cluster 5: was, the, and, of, see, movie, familiar, family, to, kids\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"labeled_imdb_reviews.csv\")\n",
        "data['Review'] = data['Review'].str.replace('[^\\w\\s]', '')  # Remove punctuation\n",
        "data['Review'] = data['Review'].str.lower()  # Convert text to lowercase\n",
        "\n",
        "# Define text representation using CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=5000, stop_words='english')\n",
        "X = vectorizer.fit_transform(data['Review'])\n",
        "\n",
        "# Apply LDA\n",
        "lda_model = LatentDirichletAllocation(n_components=10, random_state=42)\n",
        "lda_model.fit(X)\n",
        "\n",
        "# Apply LSA\n",
        "lsa_model = TruncatedSVD(n_components=10, random_state=42)\n",
        "lsa_model.fit(X)\n",
        "\n",
        "# Apply BERTopic\n",
        "\n",
        "topic_model = BERTopic(min_topic_size=3)  # Increase min_topic_size\n",
        "topics, _ = topic_model.fit_transform(data['Review'])\n",
        "\n",
        "# Get top words for each topic in LDA and LSA\n",
        "def get_top_words(model, vectorizer, n_words=10):\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for idx, topic in enumerate(model.components_):\n",
        "        top_words_idx = topic.argsort()[:-n_words - 1:-1]\n",
        "        topic_words = [words[i] for i in top_words_idx]\n",
        "        topics.append(topic_words)\n",
        "    return topics\n",
        "\n",
        "# Get top words for each topic in LDA and LSA\n",
        "lda_topics = get_top_words(lda_model, vectorizer)\n",
        "lsa_topics = get_top_words(lsa_model, vectorizer)\n",
        "\n",
        "# Print features used for topic modeling\n",
        "print(\"Features (text representation) used for topic modeling:\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print()\n",
        "\n",
        "# Print top 10 clusters/topics for LDA and LSA\n",
        "print(\"Top 10 clusters/topics for LDA:\")\n",
        "for idx, topic in enumerate(lda_topics):\n",
        "    print(f\"Cluster {idx+1}: {', '.join(topic)}\")\n",
        "    # Write a human-readable summary for each topic based on the top words and your understanding of the dataset\n",
        "print()\n",
        "\n",
        "print(\"Top 10 clusters/topics for LSA:\")\n",
        "for idx, topic in enumerate(lsa_topics):\n",
        "    print(f\"Cluster {idx+1}: {', '.join(topic)}\")\n",
        "    # Write a human-readable summary for each topic based on the top words and your understanding of the dataset\n",
        "print()\n",
        "\n",
        "# Print top topics for BERTopic\n",
        "print(\"Top clusters/topics for BERTopic:\")\n",
        "for topic_id in range(max(topics)):\n",
        "    top_words = topic_model.get_topic(topic_id)\n",
        "    top_words = [word[0] for word in top_words]  # Extracting only the words from tuples\n",
        "    print(f\"Cluster {topic_id+1}: {', '.join(top_words)}\")\n",
        "    # Write a human-readable summary for each topic based on the top words and your understanding of the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "1. Select features for the sentiment classification and explain why you select these features. Use a markdown cell to provide your explanation.\n",
        "\n",
        "2. Select two of the supervised learning algorithms/models from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build two sentiment classifiers respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "3. Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. The test set must be used for model evaluation in this step. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b51770-18e0-44b7-a820-f39a3eb91e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Multinomial Naive Bayes\n",
            "Accuracy: 0.7000\n",
            "Precision: 0.4900\n",
            "Recall: 0.7000\n",
            "F1 Score: 0.5765\n",
            "\n",
            "Model: Logistic Regression\n",
            "Accuracy: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1 Score: 1.0000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"labeled_imdb_reviews.csv\")\n",
        "\n",
        "# Split dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Review'], data['Sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Supervised learning algorithms: Multinomial Naive Bayes and Logistic Regression\n",
        "models = {\n",
        "    \"Multinomial Naive Bayes\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression()\n",
        "}\n",
        "\n",
        "# Evaluate each model\n",
        "for name, model in models.items():\n",
        "    print(f\"Model: {name}\")\n",
        "    # Train the model\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Predict on the testing data\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection is an essential step in the sentiment classification process since it helps identify the underlying patterns in text data. I've decided to employ the TF-IDF (Term Frequency-Inverse Document Frequency) features for this work. The significance of each word in a document in relation to a group of documents is represented by TF-IDF. TF-IDF successfully captures the discriminative potential of words by taking into account both a word's frequency inside a document and its rarity over the whole corpus. In order to manage the dimensionality of the feature space and reduce the possibility of overfitting, I have furthermore set a maximum feature count of 5000. By focusing on the most relevant terms and ignoring frequent stop words, this selection method improves the models' overall performance and increases their capacity to generalize to new data."
      ],
      "metadata": {
        "id": "YEu06_-1cd78"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(20 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878.\n",
        "\n",
        "1. Conduct necessary Explatory Data Analysis (EDA) and data cleaning steps on the given dataset. Split data for training and testing.\n",
        "2. Based on the EDA results, select a number of features for the regression model. Shortly explain why you select those features.\n",
        "3. Develop a regression model. The train set should be used.\n",
        "4. Evaluate performance of the regression model you developed using appropriate evaluation metrics. The test set should be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "XfvMKJjIXS5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65292f09-22ca-49cf-d310-d78abb9bd3f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in train dataset:\n",
            "Id                 0\n",
            "MSSubClass         0\n",
            "MSZoning           0\n",
            "LotFrontage      259\n",
            "LotArea            0\n",
            "                ... \n",
            "MoSold             0\n",
            "YrSold             0\n",
            "SaleType           0\n",
            "SaleCondition      0\n",
            "SalePrice          0\n",
            "Length: 81, dtype: int64\n",
            "\n",
            "Missing values in test dataset:\n",
            "Id                 0\n",
            "MSSubClass         0\n",
            "MSZoning           4\n",
            "LotFrontage      227\n",
            "LotArea            0\n",
            "                ... \n",
            "MiscVal            0\n",
            "MoSold             0\n",
            "YrSold             0\n",
            "SaleType           1\n",
            "SaleCondition      0\n",
            "Length: 80, dtype: int64\n",
            "\n",
            "Evaluation metrics for the regression model:\n",
            "Train set:\n",
            "Mean Squared Error (MSE): 1491922047.9872394\n",
            "R-squared (R2) Score: 0.7498684807747998\n",
            "\n",
            "Test set:\n",
            "Mean Squared Error (MSE): 1598354833.0864484\n",
            "R-squared (R2) Score: 0.7916184018889857\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Exploratory Data Analysis (EDA) and Data Cleaning\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values in train dataset:\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "print(\"\\nMissing values in test dataset:\")\n",
        "print(test_df.isnull().sum())\n",
        "\n",
        "# Fill missing values\n",
        "train_df.fillna(method='ffill', inplace=True)\n",
        "test_df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "# Select numerical features\n",
        "numerical_features = train_df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Split data for training and testing\n",
        "X = numerical_features.drop(columns=['SalePrice'])\n",
        "y = numerical_features['SalePrice']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Select a subset of features for the regression model\n",
        "selected_features = ['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF']\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "# Develop a regression model\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train_selected, y_train)\n",
        "\n",
        "# Evaluate performance of the regression model\n",
        "y_pred_train = regressor.predict(X_train_selected)\n",
        "y_pred_test = regressor.predict(X_test_selected)\n",
        "\n",
        "print(\"\\nEvaluation metrics for the regression model:\")\n",
        "print(\"Train set:\")\n",
        "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_train, y_pred_train))\n",
        "print(\"R-squared (R2) Score:\", r2_score(y_train, y_pred_train))\n",
        "\n",
        "print(\"\\nTest set:\")\n",
        "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred_test))\n",
        "print(\"R-squared (R2) Score:\", r2_score(y_test, y_pred_test))\n",
        "\n",
        "# Further steps could involve cross-validation, hyperparameter tuning, and additional feature engineering for model improvement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BbswDvnEX-k"
      },
      "source": [
        "# **Question 4: Using Pre-trained LLMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKwKTnW1EX-k"
      },
      "source": [
        "(20 points)\n",
        "Utilize a **Pre-trained Language Model (PLM) from the Hugging Face Repository** for predicting sentiment polarities on the data you collected in Assignment 3.\n",
        "\n",
        "Then, choose a relevant LLM from their repository, such as GPT-3, BERT, or RoBERTa or any other related models.\n",
        "1. (5 points) Provide a brief description of the PLM you selected, including its original pretraining data sources,  number of parameters, and any task-specific fine-tuning if applied.\n",
        "2. (10 points) Use the selected PLM to perform the sentiment analysis on the data collected in Assignment 3. Only use the model in the **zero-shot** setting, NO finetuning is required. Evaluate performance of the model by comparing with the groundtruths (labels you annotated) on Accuracy, Precision, Recall, and F1 metrics.\n",
        "3. (5 points) Discuss the advantages and disadvantages of the selected PLM, and any challenges encountered during the implementation. This will enable a comprehensive understanding of the chosen LLM's applicability and effectiveness for the given task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "BJgHWnOhFm-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb37fe9-8a8f-45a2-eb01-6fc5ce4888e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8235294117647058\n",
            "Precision: 0.4666666666666667\n",
            "Recall: 0.4375\n",
            "F1-score: 0.45161290322580644\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(\"labeled_imdb_reviews.csv\")\n",
        "text_data = data[\"Review\"].tolist()\n",
        "labels = data[\"Sentiment\"].tolist()\n",
        "\n",
        "# Load DistilBERT model and tokenizer\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize input text\n",
        "encoded_text = tokenizer(text_data, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**encoded_text)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "# Convert string labels to numerical labels with handling for missing labels\n",
        "label_mapping = {\"positive\": 1, \"negative\": 0}\n",
        "default_label = -1  # Default label for missing values\n",
        "numerical_labels = [label_mapping.get(label, default_label) for label in labels]\n",
        "\n",
        "# Filter out instances with default_label\n",
        "filtered_numerical_labels = [label for label in numerical_labels if label != default_label]\n",
        "filtered_predictions = [prediction for label, prediction in zip(numerical_labels, predictions) if label != default_label]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(filtered_numerical_labels, filtered_predictions)\n",
        "precision = precision_score(filtered_numerical_labels, filtered_predictions, average='macro')\n",
        "recall = recall_score(filtered_numerical_labels, filtered_predictions, average='macro')\n",
        "f1 = f1_score(filtered_numerical_labels, filtered_predictions, average='macro')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DistilBERT, more especially the \"distilbert-base-uncased-finetuned-sst-2-english\" version, is the PLM that was chosen for sentiment analysis. Hugging Face created the BERT (Bidirectional Encoder Representations from Transformers) model, which is larger and heavier than DistilBERT.\n",
        "\n",
        "This is a synopsis of the chosen DistilBERT model:\n",
        "\n",
        "Pretraining Data Sources: BooksCorpus and the English Wikipedia are two of the many text data sets that DistilBERT is pretrained on. The model may acquire broad language representations from these corpora since they have a wide variety of material from different disciplines.\n",
        "Number of Parameters: With 66 million parameters, the \"distilbert-base-uncased-finetuned-sst-2-english\" variation is computationally more efficient than the original BERT model.\n",
        "Task-Specific Fine-Tuning: The Stanford Sentiment Treebank (SST-2) dataset, which comprises of movie reviews tagged with sentiment polarity (positive or negative), is used to fine-tune the model. The pretrained DistilBERT model is adjusted through this fine-tuning process to the particular job of sentiment analysis, allowing it to pick up on task-specific patterns and subtleties in sentiment expression.\n",
        "\n",
        "Pros and Cons\n",
        "Because of its reduced size and advantages in efficiency and memory utilization, DistilBERT may be deployed in contexts with limited resources. DistilBERT, which benefits from pretrained representations acquired from extensive text corpora, maintains competitive performance on a variety of NLP tasks while having a smaller capacity than bigger models such as BERT. However, its generic pretrained representations might not adequately capture domain-specific subtleties, and its reliance on fine-tuning using labeled data may offer difficulties, particularly in areas with a dearth of labeled datasets. All things considered, DistilBERT is a flexible option for a range of NLP applications requiring a reasonable amount of processing power since it finds a balance between efficiency and performance.\n"
      ],
      "metadata": {
        "id": "3wWakAK_a6a3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VgxTP76ocMmB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}